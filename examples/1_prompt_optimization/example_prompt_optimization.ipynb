{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Settings ====\n",
    "\n",
    "enableVerboseOutput = True\n",
    "enable_context = True\n",
    "\n",
    "context_file = \"context/context_websearch.txt\"\n",
    "# context_file = \"context/context_pseudocode.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Functions ====\n",
    "\n",
    "def load_parameters(node_config):\n",
    "    global api_key\n",
    "    api_key = node_config['settings'][0]['api_key']\n",
    "\n",
    "    global model_name\n",
    "    model_name = node_config['settings'][0]['model_name']\n",
    "    global max_tokens\n",
    "    max_tokens = int(node_config['settings'][0]['max_tokens'])\n",
    "    global temperature\n",
    "    temperature = float(node_config['settings'][0]['temperature'])\n",
    "    global top_p\n",
    "    top_p = float(node_config['settings'][0]['top_p'])\n",
    "    global frequency_penalty\n",
    "    frequency_penalty = float(node_config['settings'][0]['frequency_penalty'])\n",
    "    global vecstore_path\n",
    "    vecstore_path = node_config['settings'][0]['vectorstore_path']\n",
    "    global doc_context_size\n",
    "    doc_context_size = int(node_config['settings'][0]['document_context_size'])\n",
    "    global dump_save_path\n",
    "    dump_save_path = node_config['settings'][0]['dump_save_path']\n",
    "\n",
    "    # Assemble the LLM input config\n",
    "    global llm_config\n",
    "    llm_config = {\n",
    "                    \"temp\": temperature,\n",
    "                    \"model\": model_name,\n",
    "                    \"top_p\": top_p,\n",
    "                    \"freq_penalty\": frequency_penalty,\n",
    "                    \"max_tokens\": max_tokens\n",
    "                }\n",
    "\n",
    "def gpt_inference_direct(question_input: str, config):\n",
    "\n",
    "    if question_input != \"\":\n",
    "\n",
    "        if enableVerboseOutput:\n",
    "            print(\"Running inference...\\n\")\n",
    "\n",
    "        response = openai.chat.completions.create(model=config['model'],\n",
    "                                                max_tokens=config['max_tokens'],\n",
    "                                                temperature=config['temp'],\n",
    "                                                top_p=config['top_p'],\n",
    "                                                frequency_penalty=config['freq_penalty'],\n",
    "                                                messages=[\n",
    "                                                            {\"role\": \"user\", \"content\": question_input}\n",
    "                                                        ])\n",
    "\n",
    "        response_str = response.choices[0].message.content\n",
    "        tokens_used = int(response.usage.total_tokens)\n",
    "\n",
    "        return response_str, tokens_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Load configuration file ====\n",
    "\n",
    "f = open(\"config.json\",)\n",
    "config_file = json.load(f)\n",
    "f.close()\n",
    "\n",
    "load_parameters(config_file)\n",
    "\n",
    "# Set-up OpenAI API Key\n",
    "openai.api_key = api_key\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Context loaded and will be applied on each prompt.\n"
     ]
    }
   ],
   "source": [
    "# ==== Load context ====\n",
    "\n",
    "prompt_context = \"\"\n",
    "\n",
    "# -> Load file content as instructions and append it to dynamic prompt input\n",
    "# Check if file is present\n",
    "if Path(context_file).is_file():\n",
    "    # File existing\n",
    "    tmp_file_content = \"\"\n",
    "    with open(context_file, encoding='utf8', mode='r') as f:\n",
    "        for line in f:\n",
    "            tmp_file_content += f\"{line.strip()}\\n\"\n",
    "    prompt_context = tmp_file_content\n",
    "\n",
    "    enable_context = True\n",
    "    print(\"--> Context loaded and will be applied on each prompt.\")\n",
    "\n",
    "else:\n",
    "    print(\"File not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n",
      "\n",
      "GPT >> \n",
      "1: world's tallest building 2023\n",
      "2: current tallest skyscraper 2023\n",
      "3: highest building in the world as of 2023\n",
      " \n",
      "--> Token used for request: 560\n"
     ]
    }
   ],
   "source": [
    "# ==== LLM inference ====\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "input_prompt = \"What's the world's tallest building in 2023?\"\n",
    "\n",
    "# input_prompt = \"The Apple Watch is not visible on my new iPhone and I cannot connect it. How do I configure it for the new phone?\"\n",
    "\n",
    "# input_prompt = \"My coffee machine has stopped working, no water comes out. Maybe it needs cleaning? But I don't know how to do that.\"\n",
    "\n",
    "# input_prompt = \"Please make a research about the CO2 footprint of battery electric vehicles and compare with an equivalent vehicle with a modern Diesel engine.\"\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "\n",
    "text_out = \"\"\n",
    "\n",
    "# Append context if activated\n",
    "if enable_context:\n",
    "    prompt = prompt_context + \"\\n\" + input_prompt\n",
    "\n",
    "# Inference\n",
    "text_out, tokens_used_tmp = gpt_inference_direct(prompt, llm_config)\n",
    "last_prompt = prompt\n",
    "last_response = text_out\n",
    "\n",
    "print(\"GPT >> \")\n",
    "print(text_out)\n",
    "\n",
    "if enableVerboseOutput:\n",
    "    print(\" \")\n",
    "    print(f\"--> Token used for request: {tokens_used_tmp}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
