{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "enableVerboseOutput = False\n",
    "\n",
    "def load_parameters(node_config):\n",
    "    global api_key\n",
    "    api_key = node_config['settings'][0]['api_key']\n",
    "\n",
    "    global model_name\n",
    "    model_name = node_config['settings'][0]['model_name']\n",
    "    global max_tokens\n",
    "    max_tokens = int(node_config['settings'][0]['max_tokens'])\n",
    "    global temperature\n",
    "    temperature = float(node_config['settings'][0]['temperature'])\n",
    "    global top_p\n",
    "    top_p = float(node_config['settings'][0]['top_p'])\n",
    "    global frequency_penalty\n",
    "    frequency_penalty = float(node_config['settings'][0]['frequency_penalty'])\n",
    "    global vecstore_path\n",
    "    vecstore_path = node_config['settings'][0]['vectorstore_path']\n",
    "    global doc_context_size\n",
    "    doc_context_size = int(node_config['settings'][0]['document_context_size'])\n",
    "    global dump_save_path\n",
    "    dump_save_path = node_config['settings'][0]['dump_save_path']\n",
    "\n",
    "    # Assemble the LLM input config\n",
    "    global llm_config\n",
    "    llm_config = {\n",
    "                    \"temp\": temperature,\n",
    "                    \"model\": model_name,\n",
    "                    \"top_p\": top_p,\n",
    "                    \"freq_penalty\": frequency_penalty,\n",
    "                    \"max_tokens\": max_tokens\n",
    "                }\n",
    "\n",
    "def gpt_inference_direct(question_input: str, config):\n",
    "\n",
    "    if question_input != \"\":\n",
    "\n",
    "        # Grab and print response\n",
    "        if enableVerboseOutput:\n",
    "            print(\"Running inference...\")\n",
    "\n",
    "        response = openai.chat.completions.create(model=config['model'],\n",
    "                                                max_tokens=config['max_tokens'],\n",
    "                                                temperature=config['temp'],\n",
    "                                                top_p=config['top_p'],\n",
    "                                                frequency_penalty=config['freq_penalty'],\n",
    "                                                messages=[\n",
    "                                                            {\"role\": \"user\", \"content\": question_input}\n",
    "                                                        ])\n",
    "\n",
    "        # print(response)\n",
    "        response_str = response.choices[0].message.content\n",
    "        tokens_used = int(response.usage.total_tokens)\n",
    "\n",
    "        return response_str, tokens_used\n",
    "\n",
    "def printDirect(text):\n",
    "    # Print all text at once to terminal\n",
    "    print(text)\n",
    "\n",
    "def printTw(text):\n",
    "    # Print with typewriter effect\n",
    "    for char in text:\n",
    "        sys.stdout.write(char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if char != \"\\n\":\n",
    "            time.sleep(0.1)\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "f = open(\"config.json\",)\n",
    "config_file = json.load(f)\n",
    "f.close()\n",
    "\n",
    "load_parameters(config_file)\n",
    "\n",
    "# Set-up OpenAI API Key\n",
    "openai.api_key = api_key\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "# Prompt loop for GPT Neo\n",
    "\n",
    "last_prompt = \"\"\n",
    "last_response = \"\"\n",
    "\n",
    "prompt_context = \"\"\n",
    "enable_context = False\n",
    "\n",
    "while True:\n",
    "    print(\" \")\n",
    "    prompt = input(\"Human >> \")\n",
    "    \n",
    "    if prompt == \"quit\": break\n",
    "\n",
    "    if prompt == \"clear\":\n",
    "        os.system('clear')\n",
    "        continue\n",
    "\n",
    "    if \"maxLength=\" in prompt:\n",
    "        tmp_str = prompt.split(\"=\")\n",
    "        max_length = int(tmp_str[1])\n",
    "        printDirect(f\"--> Max Length set to {str(max_length)}.\")\n",
    "        continue\n",
    "\n",
    "    if \"file=\" in prompt:\n",
    "        # -> Use the content of a file as prompt input and invoke inference\n",
    "        # Example:\n",
    "        # file=persona_4G_1.txt\n",
    "        tmp_str = prompt.split(\"=\")\n",
    "        fname = tmp_str[1]\n",
    "        # Read input text file\n",
    "        if fname:\n",
    "            # Check if file is present\n",
    "            if Path(\"prompts/\" + fname).is_file():\n",
    "                # File existing\n",
    "                tmp_file_content = \"\"\n",
    "                with open(\"prompts/\" + fname, encoding='utf8', mode='r') as f:\n",
    "                    for line in f:\n",
    "                        tmp_file_content += f\"{line.strip()}\\n\"\n",
    "                prompt = tmp_file_content\n",
    "\n",
    "            else:\n",
    "                print(\"File not found.\")\n",
    "                continue\n",
    "\n",
    "    if \"load_context=\" in prompt:\n",
    "        # -> Load file content as instructions and append it to dynamic prompt input\n",
    "        # Example:\n",
    "        # file=persona_4G_1.txt\n",
    "        tmp_str = prompt.split(\"=\")\n",
    "        fname = tmp_str[1]\n",
    "        # Read input text file\n",
    "        if fname:\n",
    "            # Check if file is present\n",
    "            if Path(\"context/\" + fname).is_file():\n",
    "                # File existing\n",
    "                tmp_file_content = \"\"\n",
    "                with open(\"context/\" + fname, encoding='utf8', mode='r') as f:\n",
    "                    for line in f:\n",
    "                        tmp_file_content += f\"{line.strip()}\\n\"\n",
    "                prompt_context = tmp_file_content\n",
    "\n",
    "                enable_context = True\n",
    "                print(\"--> Context loaded and will be applied on each prompt.\")\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                print(\"File not found.\")\n",
    "                continue\n",
    "\n",
    "    if \"personality_test=\" in prompt:\n",
    "        # Example:\n",
    "        # personality_test=persona_4G_Test.txt=personality_test_outer.txt\n",
    "        tmp_str = prompt.split(\"=\")\n",
    "        print(tmp_str)\n",
    "        fname_persona = tmp_str[1]\n",
    "        fname_test_input = tmp_str[2]\n",
    "\n",
    "        # Read persona file\n",
    "        if fname_persona:\n",
    "            # Check if file is present\n",
    "            if Path(\"prompts/\" + fname_persona).is_file():\n",
    "                # File existing\n",
    "                tmp_file_content = \"\"\n",
    "                with open(\"prompts/\" + fname_persona, encoding='utf8', mode='r') as f:\n",
    "                    for line in f:\n",
    "                        tmp_file_content += f\"{line.strip()}\\n\"\n",
    "                base_prompt = tmp_file_content\n",
    "            else:\n",
    "                print(\"Persona file not found.\")\n",
    "\n",
    "            # Read in personality test statements\n",
    "            llm_response_list = []\n",
    "            if Path(\"prompts/\" + fname_test_input).is_file():\n",
    "                # File existing\n",
    "                tmp_file_content = \"\"\n",
    "                with open(\"prompts/\" + fname_test_input, encoding='utf8', mode='r') as f:\n",
    "                    for line in f:\n",
    "                        test_statement = f\"{line.strip()}\\n\"\n",
    "                        if test_statement == \"\\n\":\n",
    "                            continue\n",
    "                        \n",
    "                        input_prompt_tmp = base_prompt +  test_statement\n",
    "\n",
    "                        # Invoke inference\n",
    "                        text_out, _ = gpt_inference_direct(input_prompt_tmp, llm_config)\n",
    "                        llm_response_list.append(text_out)\n",
    "                        time.sleep(2)\n",
    "\n",
    "                    # Write results to file\n",
    "                    with open('saved_prompts/personality_test_results.txt', 'w') as f:\n",
    "                        i = 1\n",
    "                        for line in llm_response_list:\n",
    "                            f.write(f\"{i}:\\n\")\n",
    "                            f.write(f\"{line}\\n\")\n",
    "                            f.write(\" \")\n",
    "\n",
    "                            i += 1\n",
    "\n",
    "            else:\n",
    "                print(\"Personality test file not found.\")\n",
    "\n",
    "        else:\n",
    "            print(\"File not found.\")\n",
    "            continue\n",
    "\n",
    "        continue\n",
    "\n",
    "    # Dump entire history in text file\n",
    "    if prompt == \"dump\":\n",
    "        # Generate timestamp\n",
    "        dateTimeObj = datetime.now()\n",
    "        timestampStr = dateTimeObj.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        # Assemble prompt + response\n",
    "        dump_tmp = \"prompt:\\n\"\n",
    "        dump_tmp += last_prompt + \"\\n\"\n",
    "        dump_tmp += \"\\nresponse:\\n\"\n",
    "        dump_tmp += last_response + \"\\n\"\n",
    "\n",
    "        with open(f\"{dump_save_path}/{timestampStr}.txt\", \"w\") as text_file:\n",
    "            text_file.write(dump_tmp)\n",
    "        \n",
    "        printDirect(\"--> Prompt + Response saved to text file.\")\n",
    "        continue\n",
    "\n",
    "    text_out = \"\"\n",
    "\n",
    "    # Append context if activated\n",
    "    if enable_context:\n",
    "        prompt = prompt_context + \"\\n\" + prompt\n",
    "\n",
    "    # Inference\n",
    "    text_out, tokens_used_tmp = gpt_inference_direct(prompt, llm_config)\n",
    "    last_prompt = prompt\n",
    "    last_response = text_out\n",
    "\n",
    "    print(\" \")\n",
    "    sys.stdout.write(\"GPT >> \")\n",
    "    sys.stdout.flush()\n",
    "    # printTw(text_out)\n",
    "    printDirect(text_out)\n",
    "\n",
    "    if enableVerboseOutput:\n",
    "        print(\" \")\n",
    "        printDirect(f\"--> Token used for request: {tokens_used_tmp}\")\n",
    "\n",
    "    print(\" \")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
